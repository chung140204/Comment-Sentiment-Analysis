{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b228124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def dummy_npwarn_decorator_factory():\n",
    "    def npwarn_decorator(x):\n",
    "        return x\n",
    "    return npwarn_decorator\n",
    "np._no_nep50_warning = getattr(np, '_no_nep50_warning', dummy_npwarn_decorator_factory)\n",
    "data_path_train = os.path.join(os.getcwd().replace('model', ''), 'data/processed_train.csv')\n",
    "data_path_test  = os.path.join(os.getcwd().replace('model', ''), 'data/processed_test.csv')\n",
    "train_df = pd.read_csv(data_path_train)\n",
    "test_df  = pd.read_csv(data_path_test)\n",
    "\n",
    "print(train_df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43cf358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "data_path_train = os.path.join(os.getcwd().replace('model', ''), 'data/processed_train.csv')\n",
    "train_df = pd.read_csv(data_path_train)\n",
    "\n",
    "def simple_tokenizer(text):\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "all_tokens = []\n",
    "for text in train_df['text']:\n",
    "    tokens = simple_tokenizer(text)\n",
    "    all_tokens.extend(tokens)\n",
    "\n",
    "counter = Counter(all_tokens)\n",
    "\n",
    "\n",
    "vocab = sorted(counter.keys())\n",
    "word2idx = {word: idx + 1 for idx, word in enumerate(vocab)}  \n",
    "\n",
    "print(f\"Tổng số từ duy nhất trong tập dữ liệu: {len(word2idx)}\")\n",
    "print(\"Mười từ đầu tiên trong từ điển:\")\n",
    "for word, idx in list(word2idx.items())[:10]:\n",
    "    print(f\"{word}: {idx}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c12ea17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993748df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "data_path_train = os.path.join(os.getcwd().replace('model', ''), 'data/processed_train.csv')\n",
    "data_path_test  = os.path.join(os.getcwd().replace('model', ''), 'data/processed_test.csv')\n",
    "\n",
    "train_df = pd.read_csv(data_path_train)\n",
    "test_df  = pd.read_csv(data_path_test)\n",
    "\n",
    "print(train_df.info())\n",
    "\n",
    "\n",
    "unique_labels = np.unique(train_df['sentiment'].values)\n",
    "print(\"Unique labels in training data:\", unique_labels)\n",
    "\n",
    "\n",
    "\n",
    "def simple_tokenizer(text):\n",
    "    \"\"\"\n",
    "    A simple tokenizer that lowercases text and extracts words using regex.\n",
    "    \"\"\"\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "all_tokens = []\n",
    "for text in train_df['text']:\n",
    "    tokens = simple_tokenizer(text)\n",
    "    all_tokens.extend(tokens)\n",
    "\n",
    "\n",
    "counter = Counter(all_tokens)\n",
    "\n",
    "\n",
    "vocab = sorted(counter.keys())\n",
    "\n",
    "word2idx = {word: idx + 1 for idx, word in enumerate(vocab)}\n",
    "\n",
    "print(f\"Total unique words in vocabulary: {len(word2idx)}\")\n",
    "print(\"First 10 words in vocabulary:\")\n",
    "for word, idx in list(word2idx.items())[:10]:\n",
    "    print(f\"{word}: {idx}\")\n",
    "\n",
    "\n",
    "def text_to_sequence(text, word2idx):\n",
    "    \"\"\"\n",
    "    Converts a text string into a sequence of integers based on the word2idx mapping.\n",
    "    Tokens not found in the vocabulary are mapped to 0.\n",
    "    \"\"\"\n",
    "    tokens = simple_tokenizer(text)\n",
    "    return [word2idx.get(token, 0) for token in tokens]\n",
    "\n",
    "train_sequences = [text_to_sequence(text, word2idx) for text in train_df['text']]\n",
    "test_sequences  = [text_to_sequence(text, word2idx) for text in test_df['text']]\n",
    "\n",
    "max_token_index = max(max(seq) if len(seq) > 0 else 0 for seq in train_sequences)\n",
    "expected_vocab_size = len(word2idx) + 1  # +1 for padding index (0)\n",
    "print(\"Max token index in training data:\", max_token_index)\n",
    "print(\"Expected vocabulary size (including padding):\", expected_vocab_size)\n",
    "if max_token_index >= expected_vocab_size:\n",
    "    print(\"Warning: Some token indices exceed the vocabulary size!\")\n",
    "\n",
    "max_len = max(len(seq) for seq in train_sequences)\n",
    "print(f\"Maximum sequence length: {max_len}\")\n",
    "\n",
    "def pad_sequences(sequences, max_len):\n",
    "    \"\"\"\n",
    "    Pads all sequences to the fixed length max_len.\n",
    "    Sequences longer than max_len are truncated, and shorter ones are padded with zeros.\n",
    "    \"\"\"\n",
    "    padded_seqs = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) < max_len:\n",
    "            seq = seq + [0] * (max_len - len(seq))\n",
    "        else:\n",
    "            seq = seq[:max_len]\n",
    "        padded_seqs.append(seq)\n",
    "    return np.array(padded_seqs)\n",
    "\n",
    "x_train = pad_sequences(train_sequences, max_len)\n",
    "x_test  = pad_sequences(test_sequences, max_len)\n",
    "\n",
    "\n",
    "y_train = train_df['sentiment'].values\n",
    "y_test  = test_df['sentiment'].values\n",
    "\n",
    "x_train_tensor = torch.LongTensor(x_train)\n",
    "y_train_tensor = torch.LongTensor(y_train)\n",
    "x_test_tensor  = torch.LongTensor(x_test)\n",
    "y_test_tensor  = torch.LongTensor(y_test)\n",
    "\n",
    "\n",
    "print(\"Max index in x_train_tensor:\", x_train_tensor.max().item())\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "\n",
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, max_len):\n",
    "        super(SentimentLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=0)\n",
    "        self.lstm1 = nn.LSTM(input_size=embedding_dim, hidden_size=256, batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.lstm2 = nn.LSTM(input_size=256, hidden_size=128, batch_first=True)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.lstm3 = nn.LSTM(input_size=128, hidden_size=64, batch_first=True)\n",
    "        self.fc1 = nn.Linear(64, 64)\n",
    "        self.fc2 = nn.Linear(64, 3)  # 3 classes for sentiment classification\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, max_len)\n",
    "        x = self.embedding(x)         # -> (batch_size, max_len, embedding_dim)\n",
    "        x, _ = self.lstm1(x)          # -> (batch_size, max_len, 256)\n",
    "        x = self.dropout1(x)\n",
    "        x, _ = self.lstm2(x)          # -> (batch_size, max_len, 128)\n",
    "        x = self.dropout2(x)\n",
    "        x, (h_n, _) = self.lstm3(x)   # h_n shape: (num_layers, batch_size, hidden_size)\n",
    "        last_hidden = h_n[-1]         # -> (batch_size, 64)\n",
    "        out = torch.relu(self.fc1(last_hidden))\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# Instantiate the model\n",
    "vocab_size = len(word2idx) + 1  # +1 for padding index 0\n",
    "embedding_dim = 256\n",
    "model = SentimentLSTM(vocab_size=vocab_size, embedding_dim=embedding_dim, max_len=max_len)\n",
    "print(model)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)  # Output shape: (batch_size, 3)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = correct / total\n",
    "print(\"Test Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c99949",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Giả sử bạn có các vector nhãn thật và dự đoán từ mô hình LSTM sau:\n",
    "# y_true: các nhãn thực tế (dạng numpy array hoặc list)\n",
    "# y_pred: các nhãn mô hình dự đoán (cũng dưới dạng numpy array hoặc list)\n",
    "\n",
    "# Ví dụ:\n",
    "# y_true = np.array([...])\n",
    "# y_pred = np.array([...])\n",
    "\n",
    "# Tạo báo cáo đánh giá chi tiết:\n",
    "report = classification_report(y_true, y_pred, target_names=[\"Negative\", \"Neutral\", \"Positive\"], output_dict=True)\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "print(report_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c970936f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "data_path_train = os.path.join(os.getcwd().replace('model', ''), 'data/processed_train.csv')\n",
    "data_path_test  = os.path.join(os.getcwd().replace('model', ''), 'data/processed_test.csv')\n",
    "\n",
    "train_df = pd.read_csv(data_path_train)\n",
    "test_df  = pd.read_csv(data_path_test)\n",
    "\n",
    "print(train_df.info())\n",
    "print(\"Unique sentiment labels in training data:\", np.unique(train_df['sentiment'].values))\n",
    "\n",
    "\n",
    "def simple_tokenizer(text):\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "all_tokens = []\n",
    "for text in train_df['text']:\n",
    "    tokens = simple_tokenizer(text)\n",
    "    all_tokens.extend(tokens)\n",
    "\n",
    "counter = Counter(all_tokens)\n",
    "vocab = sorted(counter.keys())\n",
    "word2idx = {word: idx + 1 for idx, word in enumerate(vocab)}\n",
    "\n",
    "print(f\"Total unique words in vocabulary: {len(word2idx)}\")\n",
    "print(\"First 10 words in vocabulary:\")\n",
    "for word, idx in list(word2idx.items())[:10]:\n",
    "    print(f\"{word}: {idx}\")\n",
    "\n",
    "def text_to_sequence(text, word2idx):\n",
    "    tokens = simple_tokenizer(text)\n",
    "    return [word2idx.get(token, 0) for token in tokens]\n",
    "\n",
    "train_sequences = [text_to_sequence(text, word2idx) for text in train_df['text']]\n",
    "test_sequences  = [text_to_sequence(text, word2idx) for text in test_df['text']]\n",
    "max_token_index = max(max(seq) if len(seq) > 0 else 0 for seq in train_sequences)\n",
    "expected_vocab_size = len(word2idx) + 1  # +1 cho padding index 0\n",
    "print(\"Max token index in training data:\", max_token_index)\n",
    "print(\"Expected vocabulary size (including padding):\", expected_vocab_size)\n",
    "\n",
    "max_len = max(len(seq) for seq in train_sequences)\n",
    "print(f\"Maximum sequence length: {max_len}\")\n",
    "\n",
    "def pad_sequences(sequences, max_len):\n",
    "    padded_seqs = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) < max_len:\n",
    "            seq = seq + [0] * (max_len - len(seq))\n",
    "        else:\n",
    "            seq = seq[:max_len]\n",
    "        padded_seqs.append(seq)\n",
    "    return np.array(padded_seqs)\n",
    "\n",
    "x_train = pad_sequences(train_sequences, max_len)\n",
    "x_test  = pad_sequences(test_sequences, max_len)\n",
    "\n",
    "\n",
    "y_train = train_df['sentiment'].values\n",
    "y_test  = test_df['sentiment'].values\n",
    "\n",
    "x_train_tensor = torch.LongTensor(x_train)\n",
    "y_train_tensor = torch.LongTensor(y_train)\n",
    "x_test_tensor  = torch.LongTensor(x_test)\n",
    "y_test_tensor  = torch.LongTensor(y_test)\n",
    "\n",
    "print(\"Max index in x_train_tensor:\", x_train_tensor.max().item())\n",
    "\n",
    "batch_size = 32\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "\n",
    "\n",
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, max_len):\n",
    "        super(SentimentLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=0)\n",
    "        self.lstm1 = nn.LSTM(input_size=embedding_dim, hidden_size=256, batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.lstm2 = nn.LSTM(input_size=256, hidden_size=128, batch_first=True)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.lstm3 = nn.LSTM(input_size=128, hidden_size=64, batch_first=True)\n",
    "        self.fc1 = nn.Linear(64, 64)\n",
    "        self.fc2 = nn.Linear(64, 3)  # 3 nhãn cảm xúc\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x, (h_n, _) = self.lstm3(x)\n",
    "        last_hidden = h_n[-1]\n",
    "        out = torch.relu(self.fc1(last_hidden))\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "vocab_size = len(word2idx) + 1\n",
    "embedding_dim = 256\n",
    "model = SentimentLSTM(vocab_size=vocab_size, embedding_dim=embedding_dim, max_len=max_len)\n",
    "print(model)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "\n",
    "\n",
    "report = classification_report(y_true, y_pred, target_names=[\"Negative\", \"Neutral\", \"Positive\"])\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3866e7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "data_path_train = os.path.join(os.getcwd().replace('model', ''), 'data/processed_train.csv')\n",
    "data_path_test  = os.path.join(os.getcwd().replace('model', ''), 'data/processed_test.csv')\n",
    "train_df = pd.read_csv(data_path_train)\n",
    "test_df  = pd.read_csv(data_path_test)\n",
    "\n",
    "print(train_df.info())\n",
    "print(\"Unique sentiment labels:\", np.unique(train_df['sentiment'].values))\n",
    "\n",
    "def simple_tokenizer(text):\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "all_tokens = []\n",
    "for text in train_df['text']:\n",
    "    tokens = simple_tokenizer(text)\n",
    "    all_tokens.extend(tokens)\n",
    "counter = Counter(all_tokens)\n",
    "vocab = sorted(counter.keys())\n",
    "word2idx = {word: idx + 1 for idx, word in enumerate(vocab)}\n",
    "\n",
    "print(f\"Total unique words in vocabulary: {len(word2idx)}\")\n",
    "print(\"First 10 words in vocabulary:\")\n",
    "for word, idx in list(word2idx.items())[:10]:\n",
    "    print(f\"{word}: {idx}\")\n",
    "\n",
    "\n",
    "def text_to_sequence(text, word2idx):\n",
    "    tokens = simple_tokenizer(text)\n",
    "    return [word2idx.get(token, 0) for token in tokens]\n",
    "\n",
    "train_sequences = [text_to_sequence(text, word2idx) for text in train_df['text']]\n",
    "test_sequences  = [text_to_sequence(text, word2idx) for text in test_df['text']]\n",
    "max_len = max(len(seq) for seq in train_sequences)\n",
    "print(f\"Maximum sequence length: {max_len}\")\n",
    "\n",
    "def pad_sequences(sequences, max_len):\n",
    "    padded_seqs = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) < max_len:\n",
    "            seq = seq + [0]*(max_len - len(seq))\n",
    "        else:\n",
    "            seq = seq[:max_len]\n",
    "        padded_seqs.append(seq)\n",
    "    return np.array(padded_seqs)\n",
    "\n",
    "x_train = pad_sequences(train_sequences, max_len)\n",
    "x_test  = pad_sequences(test_sequences, max_len)\n",
    "\n",
    "y_train = train_df['sentiment'].values\n",
    "y_test  = test_df['sentiment'].values\n",
    "\n",
    "x_train_tensor = torch.LongTensor(x_train)\n",
    "y_train_tensor = torch.LongTensor(y_train)\n",
    "x_test_tensor  = torch.LongTensor(x_test)\n",
    "y_test_tensor  = torch.LongTensor(y_test)\n",
    "\n",
    "batch_size = 32\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "test_dataset  = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "\n",
    "class ImprovedSentimentLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, max_len):\n",
    "        super(ImprovedSentimentLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=0)\n",
    "        self.lstm1 = nn.LSTM(input_size=embedding_dim, hidden_size=256, batch_first=True, bidirectional=True)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.lstm2 = nn.LSTM(input_size=512, hidden_size=128, batch_first=True, bidirectional=True)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "\n",
    "        self.lstm3 = nn.LSTM(input_size=256, hidden_size=64, batch_first=True, bidirectional=True)\n",
    "\n",
    "        self.fc1 = nn.Linear(128, 64)\n",
    "        self.fc2 = nn.Linear(64, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x, (h_n, _) = self.lstm3(x)\n",
    "        last_hidden = torch.cat((h_n[-2], h_n[-1]), dim=1)\n",
    "        out = torch.relu(self.fc1(last_hidden))\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "vocab_size = len(word2idx) + 1\n",
    "embedding_dim = 256\n",
    "model = ImprovedSentimentLSTM(vocab_size=vocab_size, embedding_dim=embedding_dim, max_len=max_len)\n",
    "print(model)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)  \n",
    "num_epochs = 20  \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "\n",
    "report = classification_report(y_true, y_pred, target_names=[\"Negative\", \"Neutral\", \"Positive\"])\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
